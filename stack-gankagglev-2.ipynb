{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# StackGAN\nAuthor: Vishal Kundar and Yash Mathur\n\nstackgan.ipynb consists of stage 1 and stage 2 code. Run the driver function runmodel() to\nexecute training. Model weights and images are stored to in directories to check progress.","metadata":{}},{"cell_type":"code","source":"#Packages\n\nimport tensorflow as tf\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n  tf.config.experimental.set_memory_growth(gpu, True)\n\nimport tensorflow.keras.backend as K\nimport numpy as np\nimport os\nfrom matplotlib import pyplot as plt\nimport pickle\nimport cv2\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-06-16T04:15:00.094801Z","iopub.execute_input":"2021-06-16T04:15:00.095168Z","iopub.status.idle":"2021-06-16T04:15:05.230159Z","shell.execute_reply.started":"2021-06-16T04:15:00.095084Z","shell.execute_reply":"2021-06-16T04:15:05.229273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_class_ids_filenames(class_id_path, filename_path):\n    with open(class_id_path, 'rb') as file:\n        class_id = pickle.load(file, encoding='latin1')\n\n    with open(filename_path, 'rb') as file:\n        filename = pickle.load(file, encoding='latin1')\n\n    return class_id, filename\n\ndef load_text_embeddings(text_embeddings):\n    with open(text_embeddings, 'rb') as file:\n        embeds = pickle.load(file, encoding='latin1')\n        embeds = np.array(embeds)\n\n    return embeds\n\ndef load_images(pickle_file):\n    #Loading images from pickle file\n    x = []\n    with open(pickle_file, 'rb') as f_in:\n        images = pickle.load(f_in)\n        \n    return images   \n\ndef parse_function(self, image_path, embeddings, bounding_box):\n    image.set_shape([64, 64, 3])\n    image = (image - 127.5) / 127.5\n\n    embedding_index = np.random.randint(0, embeddings.shape[0] - 1)\n    embedding = embeddings[embedding_index]\n    return image, embedding\n\ndef load_data(filename_path, class_id_path, embeddings_path, pickle_file):\n    \"\"\"\n    Loads the data in pickle file along with class id and embeddings\n    \"\"\"\n    class_id, filenames = load_class_ids_filenames(class_id_path, filename_path)\n    embeddings = load_text_embeddings(embeddings_path)\n    y, embeds = [], []\n\n    for i, filename in enumerate(filenames):\n        try:\n            e = embeddings[i, :, :]\n            embed_index = np.random.randint(0, e.shape[0] - 1)\n            embed = e[embed_index, :]\n\n            y.append(class_id[i])\n            embeds.append(embed)\n\n        except Exception as e:\n            print(f'{e}')\n\n    x = np.array(load_images(pickle_file))\n    y = np.array(y)\n    embeds = np.array(embeds)\n    \n    return x, y, embeds","metadata":{"execution":{"iopub.status.busy":"2021-06-16T04:15:05.231533Z","iopub.execute_input":"2021-06-16T04:15:05.231886Z","iopub.status.idle":"2021-06-16T04:15:05.245077Z","shell.execute_reply.started":"2021-06-16T04:15:05.23185Z","shell.execute_reply":"2021-06-16T04:15:05.244089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data directory for both models\n#Birds\npath = \"../input/gandata20/birds\"\ntrain_path = path + \"/train\"\ntest_path = path + \"/test\"\nembedding_train = train_path + \"/char-CNN-RNN-embeddings.pickle\"\nembedding_test = test_path + \"/char-CNN-RNN-embeddings.pickle\"\nfilename_train = train_path + \"/filenames.pickle\"\nfilename_test = test_path + \"/filenames.pickle\"\nclass_id_train = train_path + \"/class_info.pickle\"\nclass_id_test = test_path + \"/class_info.pickle\"\npickle_train_low = train_path + \"/64images.pickle\"\npickle_test_low = test_path + \"/64images.pickle\"\npickle_train_high = train_path + \"/256images.pickle\"\npickle_test_high = test_path + \"/256images.pickle\"","metadata":{"execution":{"iopub.status.busy":"2021-06-16T04:15:05.2472Z","iopub.execute_input":"2021-06-16T04:15:05.24764Z","iopub.status.idle":"2021-06-16T04:15:05.254264Z","shell.execute_reply.started":"2021-06-16T04:15:05.247597Z","shell.execute_reply":"2021-06-16T04:15:05.253361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def KL_loss(y_true, y_pred): \n  mean = y_pred[:, :128]\n  logsigma = y_pred[:, 128:]\n  loss = -logsigma + 0.5*(-1 + K.exp(2.0*logsigma) + K.square(mean))\n  loss = K.mean(loss)\n  return loss\n\nclass ConditioningAugmentation(tf.keras.Model): \n  def __init__(self):\n    super(ConditioningAugmentation, self).__init__()\n    self.dense = tf.keras.layers.Dense(units = 256)\n\n  def call(self, E):\n    X = self.dense(E)\n    phi = tf.nn.leaky_relu(X)\n    mean = phi[:, :128]\n    std = K.exp(phi[:, 128:])\n    epsilon = K.random_normal(shape = K.constant((mean.shape[1], ), dtype = 'int32'))\n    C = mean + epsilon*std\n    return C, phi\n\nclass EmbeddingCompressor(tf.keras.Model):\n  def __init__(self):\n    super(EmbeddingCompressor, self).__init__()\n    self.dense = tf.keras.layers.Dense(units = 128)\n\n  def call(self, E):\n    X = self.dense(E)\n    return tf.nn.relu(X)\n\nclass Stage1Generator(tf.keras.Model):\n  def __init__(self):\n    super(Stage1Generator, self).__init__()\n    self.canet = ConditioningAugmentation()\n    self.concat = tf.keras.layers.Concatenate(axis = 1)\n    self.dense = tf.keras.layers.Dense(units = 128*8*4*4, kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n    self.reshape = tf.keras.layers.Reshape(target_shape = (4, 4, 128*8), input_shape = (128*8*4*4, ))\n    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.deconv1 = tf.keras.layers.Conv2DTranspose(filters = 512, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.deconv2 = tf.keras.layers.Conv2DTranspose(filters = 256, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n    self.batchnorm3 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.deconv3 = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n    self.batchnorm4 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.deconv4 = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n\n  def call(self, inputs): \n    E, Z = inputs\n    C, phi = self.canet(E)\n\n    gen_input = self.concat([C, Z])\n    X = self.dense(gen_input)\n    X = self.reshape(X)\n    #X = self.batchnorm1(X)\n    X = tf.nn.relu(X)\n\n    X = self.deconv1(X)\n    X = self.batchnorm1(X)\n    X = tf.nn.relu(X)\n\n    X = self.deconv2(X)\n    X = self.batchnorm2(X)\n    X = tf.nn.relu(X)\n\n    X = self.deconv3(X)\n    X = self.batchnorm3(X)\n    X = tf.nn.relu(X)\n\n    X = self.deconv4(X)\n    return tf.nn.tanh(X), phi\n\nclass Stage1Discriminator(tf.keras.Model): \n  def __init__(self):\n    super(Stage1Discriminator, self).__init__()\n    self.conv1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.conv2 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv3 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv4 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm3 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.embed = EmbeddingCompressor()\n    self.reshape = tf.keras.layers.Reshape(target_shape = (1, 1, 128))\n    self.concat = tf.keras.layers.Concatenate()\n    self.conv5 = tf.keras.layers.Conv2D(filters = 64*8, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm4 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv6 = tf.keras.layers.Conv2D(filters = 1, kernel_size = 4, strides = 1, padding = \"valid\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n\n  def call(self, inputs):\n    I, E = inputs\n    X = self.conv1(I)\n    X = tf.nn.leaky_relu(X)\n\n    X = self.conv2(X)\n    X = self.batchnorm1(X)\n    X = tf.nn.leaky_relu(X)\n\n    X = self.conv3(X)\n    X = self.batchnorm2(X)\n    X = tf.nn.leaky_relu(X)\n\n    X = self.conv4(X)\n    X = self.batchnorm3(X)\n    X = tf.nn.leaky_relu(X)\n\n    T = self.embed(E)\n    T = self.reshape(T)\n    T = tf.tile(T, (1, 4, 4, 1))\n    merged_input = self.concat([X, T])\n\n    Y = self.conv5(merged_input)\n    Y = self.batchnorm4(Y)\n    Y = tf.nn.leaky_relu(Y)\n\n    Y = self.conv6(Y)\n    return tf.squeeze(Y)\n\nclass Stage1Model(tf.keras.Model):\n  def __init__(self):\n    super(Stage1Model, self).__init__()\n    self.stage1_generator = Stage1Generator()\n    self.stage1_discriminator = Stage1Discriminator()\n\n  def train(self, train_ds, batch_size = 128, num_epochs = 500, z_dim = 100, c_dim = 128, stage1_generator_lr = 0.0004, stage1_discriminator_lr = 0.0004):\n    generator_optimizer = tf.keras.optimizers.Adam(lr = stage1_generator_lr, beta_1 = 0.5, beta_2 = 0.999)\n    discriminator_optimizer = tf.keras.optimizers.Adam(lr = stage1_discriminator_lr, beta_1 = 0.5, beta_2 = 0.999)\n\n    for epoch in range(num_epochs):\n      print(\"Epoch %d/%d:\\n [\"%(epoch + 1, num_epochs), end = \"\")\n      start_time = time.time()\n      if epoch % 150 == 0:\n        K.set_value(generator_optimizer.learning_rate, generator_optimizer.learning_rate / 2)\n        K.set_value(discriminator_optimizer.learning_rate, discriminator_optimizer.learning_rate / 2)\n\n      generator_loss_log = []\n      discriminator_loss_log = []\n      num_batches = int(x_train.shape[0] / batch_size)\n      for i in range(num_batches):\n        image_batch = x_train[i * batch_size:(i+1) * batch_size]\n        image_batch = (image_batch - 127.5) / 127.5 \n        embedding_batch = train_embeds[i * batch_size:(i+1) * batch_size]\n        z_noise = tf.random.normal((batch_size, z_dim))\n\n        mismatched_images = tf.roll(image_batch, shift = 1, axis = 0)\n\n        real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval = 1.0)\n        fake_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n        mismatched_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n\n        with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n          fake_images, phi = self.stage1_generator([embedding_batch, z_noise])\n\n          real_logits = self.stage1_discriminator([image_batch, embedding_batch])\n          fake_logits = self.stage1_discriminator([fake_images, embedding_batch])\n          mismatched_logits = self.stage1_discriminator([mismatched_images, embedding_batch])\n\n          l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, fake_logits))\n          l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n          generator_loss = l_sup + 2.0*l_klreg\n\n          l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, real_logits))\n          l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_labels, fake_logits))\n          l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_labels, mismatched_logits))\n          discriminator_loss = 0.5*tf.add(l_real, 0.5*tf.add(l_fake, l_mismatched))\n\n        generator_gradients = generator_tape.gradient(generator_loss, self.stage1_generator.trainable_variables)\n        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, self.stage1_discriminator.trainable_variables)\n\n        generator_optimizer.apply_gradients(zip(generator_gradients, self.stage1_generator.trainable_variables))\n        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.stage1_discriminator.trainable_variables))\n\n        generator_loss_log.append(generator_loss)\n        discriminator_loss_log.append(discriminator_loss)\n\n      end_time = time.time()\n\n      if epoch % 1 == 0:\n        epoch_time = end_time - start_time\n        template = \"] - generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n        print(template.format(tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n\n      if (epoch + 1) % 50 == 0 or epoch == num_epochs - 1:\n        temp_batch_size = 1\n        temp_z_noise = tf.random.normal((temp_batch_size, z_dim))\n        temp_embedding_batch = test_embeds[0:temp_batch_size]\n        fake_images, _ = self.stage1_generator([temp_embedding_batch, temp_z_noise])\n        for i, image in enumerate(fake_images):\n          image = 127.5*image + 127.5\n          image = image.numpy().astype('uint8')\n          path = \"./gen1_\" + str(epoch + 1)  \n          cv2.imwrite(path + \"_%d.png\"%(i), image)\n\n        self.stage1_generator.save_weights(\"./stage1_generator_\" + str(epoch + 1) + \".ckpt\")\n        self.stage1_discriminator.save_weights(\"./stage1_discriminator_\" + str(epoch + 1) + \".ckpt\")","metadata":{"execution":{"iopub.status.busy":"2021-06-16T04:15:05.256083Z","iopub.execute_input":"2021-06-16T04:15:05.256443Z","iopub.status.idle":"2021-06-16T04:15:05.304842Z","shell.execute_reply.started":"2021-06-16T04:15:05.256409Z","shell.execute_reply":"2021-06-16T04:15:05.304125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stage 2","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(tf.keras.layers.Layer):\n  def __init__(self):\n    super(ResidualBlock, self).__init__()\n    self.conv1 = tf.keras.layers.Conv2D(filters = 128*4, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv2 = tf.keras.layers.Conv2D(filters = 128*4, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n\n    def call(self, I):\n      X = self.conv1(I)\n      X = self.batchnorm1(X)\n      X = tf.nn.relu(X)\n\n      X = self.conv2(X)\n      X = self.batchnorm2(X)\n      X = tf.nn.relu(X)\n      X = tf.keras.layers.Add()([X, I])\n      X = tf.nn.relu(X)\n      return X\n\nclass Stage2Generator(tf.keras.Model):\n  def __init__(self):\n    super(Stage2Generator, self).__init__()\n    self.canet = ConditioningAugmentation()\n    self.conv1 = tf.keras.layers.Conv2D(128, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.conv2 = tf.keras.layers.Conv2D(256, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv3 = tf.keras.layers.Conv2D(512, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv4 = tf.keras.layers.Conv2D(512, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm3 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.resblock1 = ResidualBlock()\n    self.resblock2 = ResidualBlock()\n    self.resblock3 = ResidualBlock()\n    self.resblock4 = ResidualBlock()\n    self.upsamp1 = tf.keras.layers.UpSampling2D(size = (2, 2))\n    self.conv5 = tf.keras.layers.Conv2D(256, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm4 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.upsamp2 = tf.keras.layers.UpSampling2D(size = (2, 2))\n    self.conv6 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm5 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.upsamp3 = tf.keras.layers.UpSampling2D(size = (2, 2))\n    self.conv7 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm6 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.upsamp4 = tf.keras.layers.UpSampling2D(size = (2, 2))\n    self.conv8 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm7 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv9 = tf.keras.layers.Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n  \n  def call(self, inputs):\n    E, I = inputs\n    C, phi = self.canet(E)\n\n    X = self.conv1(I)\n    X = tf.nn.relu(X)\n    \n    X = self.conv2(X)\n    X = self.batchnorm1(X)\n    X = tf.nn.relu(X)\n\n    X = self.conv3(X)\n    X = self.batchnorm2(X)\n    X = tf.nn.relu(X)\n\n    C = K.expand_dims(C, axis = 1)\n    C = K.expand_dims(C, axis = 1)\n    C = K.tile(C, [1, 16, 16, 1])\n    J = K.concatenate([C, X], axis = 3)\n\n    X = self.conv4(X)\n    X = self.batchnorm3(X)\n    X = tf.nn.relu(X)\n\n    X = self.resblock1(X)\n    X = self.resblock2(X)\n    X = self.resblock3(X)\n    X = self.resblock4(X)\n\n    X = self.upsamp1(X)\n    X = self.conv5(X)\n    X = self.batchnorm4(X)\n    X = tf.nn.relu(X)\n    \n    X = self.upsamp2(X)\n    X = self.conv6(X)\n    X = self.batchnorm5(X)\n    X = tf.nn.relu(X)\n    \n    X = self.upsamp3(X)\n    X = self.conv7(X)\n    X = self.batchnorm6(X)\n    X = tf.nn.relu(X)\n    \n    X = self.upsamp4(X)\n    X = self.conv8(X)\n    X = self.batchnorm7(X)\n    X = tf.nn.relu(X)\n    \n    X = self.conv9(X)\n    return tf.nn.tanh(X), phi\n\nclass Stage2Discriminator(tf.keras.Model):\n  def __init__(self):\n    super(Stage2Discriminator, self).__init__()\n    self.embed = EmbeddingCompressor()\n    self.reshape = tf.keras.layers.Reshape(target_shape = (1, 1, 128))\n    self.conv1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.conv2 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv3 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv4 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm3 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv5 = tf.keras.layers.Conv2D(filters = 1024, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm4 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv6 = tf.keras.layers.Conv2D(filters = 2048, kernel_size = 4, strides = 2, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm5 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv7 = tf.keras.layers.Conv2D(filters = 1024, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm6 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv8 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm7 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv9 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm8 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv10 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm9 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv11 = tf.keras.layers.Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm10 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv12 = tf.keras.layers.Conv2D(filters = 64*8, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n    self.batchnorm11 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n    self.conv13 = tf.keras.layers.Conv2D(filters = 1, kernel_size = 4, strides = 1, padding = \"valid\", kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = 0.02))\n\n  def call(self, inputs):\n    I, E = inputs\n    T = self.embed(E)\n    T = self.reshape(T)\n    T = tf.tile(T, (1, 4, 4, 1))\n\n    X = self.conv1(I)\n    X = tf.nn.leaky_relu(X)\n\n    X = self.conv2(X)\n    X = self.batchnorm1(X)\n    X = tf.nn.leaky_relu(X)\n    \n    X = self.conv3(X)\n    X = self.batchnorm2(X)\n    X = tf.nn.leaky_relu(X)\n    \n    X = self.conv4(X)\n    X = self.batchnorm3(X)\n    X = tf.nn.leaky_relu(X)\n    \n    X = self.conv5(X)\n    X = self.batchnorm4(X)\n    X = tf.nn.leaky_relu(X)\n   \n    X = self.conv6(X)\n    X = self.batchnorm5(X)\n    X = tf.nn.leaky_relu(X)\n    \n    X = self.conv7(X)\n    X = self.batchnorm6(X)\n    X = tf.nn.leaky_relu(X)\n    \n    X = self.conv8(X)\n    X = self.batchnorm7(X)\n\n    Y = self.conv9(X)\n    Y = self.batchnorm8(Y)\n    Y = tf.nn.leaky_relu(Y)\n\n    Y = self.conv10(Y)\n    Y = self.batchnorm9(Y)\n    Y = tf.nn.leaky_relu(Y)\n\n    Y = self.conv11(Y)\n    Y = self.batchnorm10(Y)\n\n    A = tf.keras.layers.Add()([X, Y])\n    A = tf.nn.leaky_relu(A)\n\n    merged_input = tf.keras.layers.concatenate([A, T])\n\n    Z = self.conv12(merged_input)\n    Z = self.batchnorm11(Z)\n    Z = tf.nn.leaky_relu(Z)\n    \n    Z = self.conv13(Z)\n    return tf.squeeze(Z)\n\nclass Stage2Model(tf.keras.Model):\n  def __init__(self):\n    super(Stage2Model, self).__init__()\n    self.stage1_generator = Stage1Generator()\n    self.stage1_generator.compile(loss = \"mse\", optimizer = \"adam\")\n    self.stage1_generator.load_weights(\"../input/allweights/stage1_generator_300.ckpt\").expect_partial()\n    \n    self.stage2_generator = Stage2Generator()\n    self.stage2_discriminator = Stage2Discriminator()\n    self.stage2_generator.compile(loss = \"mse\", optimizer = \"adam\")\n    self.stage2_generator.load_weights(\"../input/allweights/stage2_generator_101.ckpt\").expect_partial()\n    self.stage2_discriminator.compile(loss = \"mse\", optimizer = \"adam\")\n    self.stage2_discriminator.load_weights(\"../input/allweights/stage2_discriminator_101.ckpt\").expect_partial()\n    \n  def train(self, train_ds, batch_size = 64, num_epochs = 800, z_dim = 100, stage1_generator_lr = 0.0001, stage1_discriminator_lr = 0.0001):\n    generator_optimizer = tf.keras.optimizers.Adam(lr = stage1_generator_lr, beta_1 = 0.5, beta_2 = 0.999)\n    discriminator_optimizer = tf.keras.optimizers.Adam(lr = stage1_discriminator_lr, beta_1 = 0.5, beta_2 = 0.999)\n    \n    for epoch in range(num_epochs):\n      print(\"Epoch %d/%d:\\n [\"%(epoch + 1, num_epochs), end = \"\")\n      start_time = time.time()\n      if epoch % 100 == 0:\n        K.set_value(generator_optimizer.learning_rate, generator_optimizer.learning_rate / 2)\n        K.set_value(discriminator_optimizer.learning_rate, discriminator_optimizer.learning_rate / 2)\n    \n      generator_loss_log = []\n      discriminator_loss_log = []\n      num_batches = int(x_train.shape[0] / batch_size)\n      for i in range(num_batches):\n        if i % 5 == 0:\n          print(\"=\", end = \"\")\n        \n        image_batch = x_train[i * batch_size:(i+1) * batch_size]\n        hr_image_batch = (image_batch - 127.5) / 127.5 \n        embedding_batch = train_embeds[i * batch_size:(i+1) * batch_size]\n        z_noise = tf.random.normal((batch_size, z_dim))\n\n        mismatched_images = tf.roll(hr_image_batch, shift = 1, axis = 0)\n\n        real_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.9, maxval = 1.0)\n        fake_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n        mismatched_labels = tf.random.uniform(shape = (batch_size, ), minval = 0.0, maxval = 0.1)\n\n        with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n          lr_fake_images, _ = self.stage1_generator([embedding_batch, z_noise])\n          hr_fake_images, phi = self.stage2_generator([embedding_batch, lr_fake_images])\n          real_logits = self.stage2_discriminator([hr_image_batch, embedding_batch])\n          fake_logits = self.stage2_discriminator([hr_fake_images, embedding_batch])\n          mismatched_logits = self.stage2_discriminator([mismatched_images, embedding_batch])\n          \n          l_sup = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, fake_logits))\n          l_klreg = KL_loss(tf.random.normal((phi.shape[0], phi.shape[1])), phi)\n          generator_loss = l_sup + 2.0*l_klreg\n          \n          l_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_labels, real_logits))\n          l_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_labels, fake_logits))\n          l_mismatched = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(mismatched_labels, mismatched_logits))\n          discriminator_loss = 0.5*tf.add(l_real, 0.5*tf.add(l_fake, l_mismatched))\n        \n        generator_gradients = generator_tape.gradient(generator_loss, self.stage2_generator.trainable_variables)\n        discriminator_gradients = discriminator_tape.gradient(discriminator_loss, self.stage2_discriminator.trainable_variables)\n        \n        generator_optimizer.apply_gradients(zip(generator_gradients, self.stage2_generator.trainable_variables))\n        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.stage2_discriminator.trainable_variables))\n        \n        generator_loss_log.append(generator_loss)\n        discriminator_loss_log.append(discriminator_loss)\n        \n      end_time = time.time()\n\n      if epoch % 1 == 0:\n        epoch_time = end_time - start_time\n        template = \"] - generator_loss: {:.4f} - discriminator_loss: {:.4f} - epoch_time: {:.2f} s\"\n        print(template.format(tf.reduce_mean(generator_loss_log), tf.reduce_mean(discriminator_loss_log), epoch_time))\n\n      if epoch % 50 == 0 or epoch == num_epochs - 1:\n        save_path = \"./\"\n        temp_batch_size = 5\n        temp_z_noise = tf.random.normal((temp_batch_size, z_dim))\n        temp_embedding_batch = test_embeds[0:temp_batch_size]\n        lr_fake_images, _ = self.stage1_generator([temp_embedding_batch, temp_z_noise])\n        hr_fake_images, _ = self.stage2_generator([temp_embedding_batch, lr_fake_images])\n        for i, image in enumerate(hr_fake_images):\n          image = 127.5*image + 127.5\n          image = image.numpy().astype('uint8')\n          cv2.imwrite(save_path + \"gen2_%d.png\"%(i), image)\n        self.stage2_generator.save_weights(\"./stage2_generator_\" + str(epoch + 1) + \".ckpt\")\n        self.stage2_discriminator.save_weights(\"./stage2_discriminator_\" + str(epoch + 1) + \".ckpt\")","metadata":{"execution":{"iopub.status.busy":"2021-06-16T04:15:05.307959Z","iopub.execute_input":"2021-06-16T04:15:05.308211Z","iopub.status.idle":"2021-06-16T04:15:05.376708Z","shell.execute_reply.started":"2021-06-16T04:15:05.308188Z","shell.execute_reply":"2021-06-16T04:15:05.375955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, y_train, train_embeds = load_data(filename_path=filename_train, class_id_path=class_id_train, embeddings_path=embedding_train, pickle_file = pickle_train_high)\nx_test, y_test, test_embeds = load_data(filename_path=filename_test, class_id_path=class_id_test, embeddings_path=embedding_test, pickle_file = pickle_test_high)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T04:15:05.379297Z","iopub.execute_input":"2021-06-16T04:15:05.379788Z","iopub.status.idle":"2021-06-16T04:15:35.874087Z","shell.execute_reply.started":"2021-06-16T04:15:05.379756Z","shell.execute_reply":"2021-06-16T04:15:35.873083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_model():\n    ####################\n    #STAGE 1\n    ####################\n    \"\"\"\n    model = Stage1Model() \n    #model.train(0, num_epochs = 1)\n    model.stage1_generator.load_weights(\"../input/allweights/stage1_generator_300.ckpt\").expect_partial()\n    model.stage1_discriminator.load_weights(\"../input/allweights/stage1_discriminator_300.ckpt\").expect_partial()\n    model.train(0)\n    \"\"\"\n    ####################\n    #STAGE 2\n    ####################\n    model = Stage2Model()\n    model.train(0)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T04:15:35.875522Z","iopub.execute_input":"2021-06-16T04:15:35.875894Z","iopub.status.idle":"2021-06-16T04:15:35.882341Z","shell.execute_reply.started":"2021-06-16T04:15:35.875857Z","shell.execute_reply":"2021-06-16T04:15:35.881225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_model()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T04:15:35.884831Z","iopub.execute_input":"2021-06-16T04:15:35.88519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stage1_generateImage(z_dim=100, samples=1):\n    model = Stage1Model()\n    #model.train(0, num_epochs = 1)\n    model.stage1_generator.load_weights(\"../input/allweights/stage1_generator_300.ckpt\").expect_partial()\n    model.stage1_discriminator.load_weights(\"../input/allweights/stage1_discriminator_300.ckpt\").expect_partial()\n    temp_batch_size = samples\n    temp_z_noise = tf.random.normal((temp_batch_size, z_dim))\n    \n    temp_embedding_batch = test_embeds[0:temp_batch_size]\n    print(\"Embeddings: \", temp_embedding_batch)\n    \n    fake_images, _ = model.stage1_generator([temp_embedding_batch, temp_z_noise])\n    for i, image in enumerate(fake_images):\n        image = 127.5*image + 127.5\n        image = image.numpy().astype('uint8')\n        \n        img = plt.figure()\n        ax = img.add_subplot(1,1,1)\n        ax.imshow(image)  \n        #path = \"./gen1_results\"\n        #cv2.imwrite(path + \"_%d.png\"%(i), image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #stage1_generateImage(samples=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}